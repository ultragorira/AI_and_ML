{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9996459484100342}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline('sentiment-analysis')\n",
    "clf('This is terrible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stages in pipeline\n",
    "\n",
    "- Tokenizer => Raw Text turned into numbers\n",
    "- Model => Input from Tokenizer turned into logits\n",
    "- Postprocessing => Logits turned into predictions\n",
    "\n",
    "Tokenization steps are several steps. \n",
    "First it will split each word, part of text or punctuation, creating the tokens.\n",
    "Then it will add in front a CLS token and SEP token at the end. \n",
    "Then these tokens are turned into numbers mapped to the vocabulary of trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 2204,  102,    0,    0,    0,    0],\n",
      "        [ 101, 1045, 2079, 2025, 2066, 2023, 2518, 2158,  102],\n",
      "        [ 101, 2129, 8038, 2725,  102,    0,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english' #=>Default one\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_data = ['This is good', 'I do not like this thing man', 'How ya doing']\n",
    "\n",
    "'''\n",
    "    Tokenizer inputs:\n",
    "    raw data\n",
    "    padding => This will add 0s to sentences that are shorter than the max length\n",
    "    truncation => This will truncate sentences that are longer than the max length allowed by the model\n",
    "    return_tensors => Specify what type of tensor to receive back, pt is for PyTorch\n",
    "'''\n",
    "inputs = tokenizer(raw_data, padding=True, truncation=True, return_tensors='pt')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor returned has batch size 3 (This depends on the raw data), sequence lenght 9 (This will depend on the raw data), and hidden dimension 768.\n",
    "The model heads take in this tensor as input and project it to a different dimension.\n",
    "\n",
    "<img src='pics/ModelHeads.png'>\n",
    "\n",
    "\n",
    "Depending on the task, there are different architectures available:\n",
    "\n",
    "*Model (retrieve the hidden states)\n",
    "*ForCausalLM\n",
    "*ForMaskedLM\n",
    "*ForMultipleChoice\n",
    "*ForQuestionAnswering\n",
    "*ForSequenceClassification\n",
    "*ForTokenClassification\n",
    "\n",
    "and more\n",
    "\n",
    "For a text classification task, we can use the *ForSequenceClassification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1668,  4.5490],\n",
      "        [ 2.7439, -2.3426],\n",
      "        [-2.2669,  2.3813]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output returned is a 2x2 vector with logits. These logits need to be processed so that we can get the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6393e-04, 9.9984e-01],\n",
       "        [9.9386e-01, 6.1418e-03],\n",
       "        [9.4882e-03, 9.9051e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PostProcessing\n",
    "\n",
    "import torch\n",
    "\n",
    "pred = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits are now convertged and numbers are between 0 and 1.<br>\n",
    "Let's check the model configuration for the labels 0 and 1 <br>\n",
    "In this case it means the first sentence is POS as the second label (1) has a probability of 0.99984 to be POS and 0.000164 to be NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b387498046005d87260c2a15af8957b404ae359b8249b6aaf657658f1b4409b6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
