{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups of Transformers:\n",
    "\n",
    "Auto-Regressive model like GPT <br>\n",
    "Auto-Encoding model like BERT <br>\n",
    "Sequence-To-Sequence model like BART or T5\n",
    "\n",
    "Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition. <br>\n",
    "Decoder-only models: Good for generative tasks such as text generation.<br>\n",
    "Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.\n",
    "\n",
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "<img src=\"pics/Transformer_Architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Models\n",
    "\n",
    "BERT, ALBERT, DistilBERT, ELECTRA, RoBERTa are one of the most popular Encoder Models.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "Encoder models have bi-directional attention. They are able to contextcualize the meaning of a word based on the context.\n",
    "These encoders are often called auto-enconding models.\n",
    "\n",
    "The encoder, for a sequence of words, outputs a numerical rapresentation of each word. The vector rapresent not only the word but also the word considering the surrounding context. This numerical representation is also called features tensor or vector.\n",
    "The size of the tensor depends on the architecture. In BERT it 768.\n",
    "Remember: 1 tensor per input word is the output. \n",
    "\n",
    "Encoders are suitable for tasks where bi-directional attention is required, for example MLM (Mask Language Models) or Sentiment Analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Models\n",
    "\n",
    "GPT, GPT-2 are examples of a Decoder Model.\n",
    "\n",
    "Decoder have similarities with Encoder models however the Decoder models are uni-directional, meaning that they do not use context of all surrounding words but only from left (of right) to predict the next word. These models are often called auto-regressive models. GPT is able to generate up to 1024 words and it can still remember about the first word of the sequence. Example of how the Decoder works:\n",
    "\n",
    "<img src='pics/DecoderExample.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence (Encoder-Decoder)\n",
    "\n",
    "T5 and BART are known Sequence to Sequence Models.\n",
    "\n",
    "The input, a sequence, is passed to the encoder which outputs a numerical rapresentation of the it. This output is then passed to the decoder which will use it to predict the next word. Along side this encoder-input, the decoder uses the word predicted as input, here comes in the auto-regressive characteristic of the decoder.\n",
    "\n",
    "The Encoder-Decoder models shine in tasks like translation and summarization.\n",
    "The weights learnt by the Encoder and Decoder are not necessarely shared accross them.\n",
    "\n",
    "Example of how the Encoder-Decoder works:\n",
    "\n",
    "<img src='pics/EncoderDecoder0.png'>\n",
    "\n",
    "<img src='pics/EncoderDecoder1.png'>\n",
    "\n",
    "<img src='pics/EncoderDecoder2.png'>\n",
    "\n",
    "<img src='pics/EncoderDecoder3.png'>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b387498046005d87260c2a15af8957b404ae359b8249b6aaf657658f1b4409b6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
