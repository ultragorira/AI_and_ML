{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "#### Word-based tokenizer\n",
    "\n",
    "With Word-based tokenizer a sentence can be split by spaces or by punctuation for example.\n",
    "\n",
    "I'm going to the cinema => will be 5 tokens if split by spaces.\n",
    "\n",
    "The word-based tokenization has some limitations, for example \"house\" and \"houses\" will have each its own number representation although the words are similar just that one word is the plural of the other one. Another limitation is that if you want the model to understand everything of that particular language, the vocabulary size becomes very large and heavy.\n",
    "One way to solve this is to actually work with the 10k most common words. This will mean that if there is a word in the sentence that does not belong to the vocabulary, so called out of vocabulary, this word will be transformed into an UNKNOWN tag which can be a problem if for example there is more than one word in the same sentence that does not belong to the vocabulary.\n",
    "\n",
    "#### Character-based tokenizer\n",
    "\n",
    "170entence can be split by characters. This method reduces drastically the vocabulary size. In the Word-based tokenization, for English a whole vocabulary would be ~170k words, whereas the character-based tokenization would have 256 including special characters and punctuation.\n",
    "With character-based tokenization there may be a loss of context as a letter by itself does not really mean much. This is true depending on the language. For example for Chinese a single character can have a lot of meaning, while in a latin language no. \n",
    "\n",
    "Another issue with character-based tokenization is that the number of tokens will be large whereas with word-based tokenization for a sentence you would have maybe 5 tokens and 40-50 for characters based.\n",
    "\n",
    "\n",
    "#### Subword tokenization\n",
    "\n",
    "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. WordPiece is a type of Subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 502kB/s] \n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 4.94kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 752kB/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8790, 1175, 1139, 1271, 1110, 23255, 117, 1293, 1336, 146, 1494, 1128, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hi there my name is Jarvis, how may I help you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('folder_of_your_choice\\\\tokenizer_config.json',\n",
       " 'folder_of_your_choice\\\\special_tokens_map.json',\n",
       " 'folder_of_your_choice\\\\vocab.txt',\n",
       " 'folder_of_your_choice\\\\added_tokens.json',\n",
       " 'folder_of_your_choice\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To save tokenizer\n",
    "\n",
    "tokenizer.save_pretrained(\"folder_of_your_choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'does', 'token', '##ization', 'work', '?']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"How does tokenization work?\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1731, 1674, 22559, 2734, 1250, 136]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert tokens to numbers\n",
    "to_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How does tokenization work?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decode the tokens\n",
    "\n",
    "decoded = tokenizer.decode(to_ids)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Here you can see the shorter sentences was padded with 0\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b387498046005d87260c2a15af8957b404ae359b8249b6aaf657658f1b4409b6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
