{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagram of how Reinforcement Learning works\n",
    "\n",
    "<img src=\"pics/RL_Diagram.png\">\n",
    "\n",
    "<img src=\"pics/RL_Steps.png\">\n",
    "\n",
    "In this case the state would be the frame of the environment.\n",
    "\n",
    "Reinforcement Learning is based on the Reward Hypothesis where all goals can be described as the maximization of the expected cumulative reward (Expected Return)\n",
    "\n",
    "\n",
    "***STATE***:\n",
    "\n",
    "It is when you have a complete description of the state of the world, no hidden information, for example a chess board.\n",
    "It is instead called Observation when you have a partial description of the state of the world, for example Super Mario where not the whole world is visible at once. T\n",
    "\n",
    "***ACTION***:\n",
    "\n",
    "It can be Discrete when a number of finite possible actions can be taken, for example in Super Mario right, left, up, down and jump.\n",
    "It can be Continuous when an infinite possible actions can be taken, for example a self driving car.\n",
    "\n",
    "***REWARD***:\n",
    "\n",
    "The only feedback for the agent. Depending on the reward, negative or positive, the agent knows if the action was good or bad.\n",
    "The reward is the cumulative of all the actions. It can be written like this:\n",
    "\n",
    "<img src=\"pics/CumulativeReward.png\">\n",
    "<img src=\"pics/CumulativeReward2.png\">\n",
    "\n",
    "However when it comes to rewards, discounting of the rewards is done. Normally we define a Discount Rate, called gamma, which is between 0 and 1.\n",
    "The higher the gamma, the smaller the reward so the agent will care about long terms rewards, the smaller the gamma the more the agent will care about short terms rewards.\n",
    "Additionally we multiply the reward by the gamma to the exponent of the timestep.\n",
    "\n",
    "<img src=\"pics/CumulativeReward_Gamma.png\">\n",
    "\n",
    "\n",
    "### Type of tasks\n",
    "\n",
    "EPISODIC: When there is a start and end point. For example in Super Mario you start the game and it ends when you are killed or arrive at the end of the level. There is a terminal state. \n",
    "\n",
    "CONTINUOUS: When the task can go on forever, no terminal state.\n",
    "\n",
    "\n",
    "### Exploration vs Exploitation\n",
    "\n",
    "With Exploration the agent will take random actions to explore the environment. With Exploitation the agen will use known information to maximize reward. \n",
    "Exploitation can be interpreted as going always to the same restaurant but know that is good and possibly missing a better option. \n",
    "Exploration on the other hand is like going to different restaurants taking the risk of having bad experience but have more opportunities to have better experiences. \n",
    "\n",
    "\n",
    "### POLICY\n",
    "\n",
    "Found during training is basically the brain of the Agent. This is to maximize the expected reward. \n",
    "\n",
    "Policy-Based method: train an agent to learn which action to take given a state\n",
    "Value-Based method: train an agent to learn wich state is more valuable and take the action that leads to it. \n",
    "\n",
    "<img src=\"pics/Policy-BasedVS-Value-Based.png\">\n",
    "\n",
    "We have two types of policy:\n",
    "\n",
    "Deterministic: a policy at a given state will always return the same action.\n",
    "Stochastic: output a probability distribution over actions."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b387498046005d87260c2a15af8957b404ae359b8249b6aaf657658f1b4409b6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
