{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All pics are taken from https://huggingface.co/blog/deep-rl-q-part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Based method\n",
    "\n",
    "Indirectly, by training a value function that outputs the value of a state or a state-action pair. Given this value function, the policy will take action.\n",
    "\n",
    "In Value Based methods we do not train the policy, the policy is a function defined by hand. We train a value function that is a Neural Network. \n",
    "\n",
    "So no matter the method, you will still have a policy but in case in value mathods, the policy is just a function that is specified, as a greedyt policy. \n",
    "\n",
    "Main differences:\n",
    "\n",
    "In policy based, the optimal policy is found by training the policy directly.\n",
    "In value based, finding an optimal value function leads to having an optimal policy.\n",
    "\n",
    "In most cases, an Epsillon-Greedy Policy is used to handle the exploration and exploitation.\n",
    "\n",
    "### Two type of value based functions\n",
    "\n",
    "***State-Value function***\n",
    "For each state, the state value function outputs the expected return if the agent starts at that state and then follow the policy forever after.\n",
    "\n",
    "<img src='pics/state-value-function-1.jpg'>\n",
    "\n",
    "The state value function calculates the value of a state\n",
    "\n",
    "***Action-Value function***\n",
    "\n",
    "For each state and action pair, the action value function outputs the expected return if the agent starts in that state and takes action, and then follows the policy forever after. \n",
    "\n",
    "<img src='pics/action-state-value-function-1.jpg'>\n",
    "\n",
    "\n",
    "### Main difference between State Value and Action Value functions ###\n",
    "\n",
    "In State Value Function we calculate the value of the state St\n",
    "In Action Value Function we calculate the value of the state-action pair St and At, so the value of taking the action at that state. \n",
    "\n",
    "<img src='pics/two-types.jpg'>\n",
    "\n",
    "In both we calculate the expected value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation ###\n",
    "\n",
    "For both State and Action Value based functions, we need to calculate for each of the state or action-state pair, the sum of all the rewards an agent can get if the agent started at that state. This can be a very lenghty process and the Bellman Equation comes in handy.\n",
    "\n",
    "\n",
    "The Bellman Equation is a recurisve equation.\n",
    "\n",
    "<img src='pics/bellman.jpg'>\n",
    "\n",
    "Immediate reward Rt+1 plus the discounted value of the state that follows (gamma * V(St+1))\n",
    "\n",
    "The main idea is that instead of calculating each value as the sum of the expected return, we calculate the sum of the immediate reward and the discounted value of the state that follows. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo vs TDL (Temporal Difference Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo and TDL are two ways of training the value function or the policy function. Both of them use experience to solve RL problems.\n",
    "\n",
    "With Monte Carlo we use an entier episode of experience before learning, while TDL uses only a step to learn.\n",
    "\n",
    "##### Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/monte-carlo-approach.jpg'>\n",
    "\n",
    "With Monte Carlo we always start at the same starting point. The agent takes actions based on the policy.\n",
    "We check if the episode is finished, e.g. mouse got eaten by cat, and at the end of the episode we will have a list of States, Actions, Rewards and Next steps.\n",
    "The agent will sump the total rewards Gt and see how good it did.\n",
    "V(st) in the formula will be updated.\n",
    "\n",
    "Start new episode with this new knowledge. The more episodes, the more the agent will learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDL Temporal Distance Learning\n",
    "\n",
    "In TDL, we do not wait for a full episode to finish but wait only for one interaction/step St+1\n",
    "\n",
    "In TDL we update V(St) at each step.\n",
    "However since we did not go through a full episode, we do not have Gt (expected return) but we estimate Gt by adding Rt+1 and the discounted value of the next step.\n",
    "\n",
    "<img src='pics/TDL.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Monte Carlo we have the actual accurate discounted return of the episode and update the function only after the episode is complete. This\n",
    "\n",
    "With TDL, the function is updated at each step and replace Gt (expected return) with an estimated return called TD target.\n",
    "\n",
    "<img src='pics/Difference_MonteCarlo_TDL.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is an off policy value-based method that uses Temporal Distance approach to train its action-value function. \n",
    "Q-Learning algorithm is used to train the Q-function which is action-bases (pair of action and state) to determined the value of being at a particular state and taking a specific action at that state. \n",
    "Q is meant for Quality of the action of that state. \n",
    "\n",
    "Given a pair of state and action, the Q function will output a state-action value, called also Q-value.\n",
    "The function has a Q-table, which acts like memory of the function where each cell corresponds to a state-action value pair. \n",
    "\n",
    "Once the training is done, we have an optimal Q-Table.\n",
    "When we have an optimal Q-function, we have an optimal policy as for each state we know what is the best action to take. \n",
    "\n",
    "### Steps Q-Learning Algorithm\n",
    "\n",
    "The table is initialized for each pair state-action. Normally initialization is with 0.\n",
    "Choose an action using Epsilon Greedy Strategy. Epsiplon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.\n",
    "\n",
    "The idea is that we define epsilon ɛ = 1.0:\n",
    "With probability 1 — ɛ : we do exploitation (aka our agent selects the action with the highest state-action pair value).\n",
    "With probability ɛ: we do exploration (trying random action).\n",
    "\n",
    "At the beginning we will do a lot of exploration as epsilon is high. With more training and Q-table updated, there will be less and less exploration and more of exploitation.\n",
    "\n",
    "<img src='pics/Epsilon.jpg'>\n",
    "\n",
    "Perform action At, get reward Rt+1 and next state St+1\n",
    "\n",
    "Update Q(St and At)\n",
    "\n",
    "<img src=\"pics/Update_Q.jpg\">\n",
    "\n",
    "To update Q(St, At) we need St, At, Rt+1, St+1.\n",
    "To update the Q-value we need the TD target.\n",
    "\n",
    "\n",
    "### Difference between off-policy and on-policy\n",
    "\n",
    "With off-policy we use different policy for acting and updating while for on-policy we use the same policy for everything. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Q-Learning\n",
    "\n",
    "<img src=\"pics/Q-LearningExample.jpg\">\n",
    "\n",
    "The goal is for the mouse to eat the big pile of cheese on the bottom right.\n",
    "\n",
    "The episode finishes if the mouse eats the poison, eats the big pile of cheese or does more than 5 steps.\n",
    "\n",
    "Learning rate is 0.1\n",
    "\n",
    "Gamma is 0.99\n",
    "\n",
    "The reward function will have different returns, +1 if eat cheese, -10 if eat poison, +0 if no cheese or more than 5 steps, +10 if eat the big pile.\n",
    "\n",
    "\n",
    "The table is initialized with zero.\n",
    "\n",
    "<img src=\"pics/QLearningTable.jpg\">\n",
    "\n",
    "Now we start the training with Q-Learning in two timesteps.\n",
    "\n",
    "Timestep 1, we choose an action using the Epsilon Greedy Strategy. The value of eplison is high=1 so we take a random action, exploration.\n",
    "We go to right.\n",
    "\n",
    "<img src=\"pics/GoRight.jpg\">\n",
    "\n",
    "We perform the action At and we get Rt+1 and St+1\n",
    "\n",
    "We are in a new state now.\n",
    "\n",
    "<img src=\"pics/NewState.jpg\">\n",
    "\n",
    "We update Q(St, At)\n",
    "<img src=\"pics/UpdateQ.jpg\">\n",
    "\n",
    "<img src=\"pics/UpdateQ2.jpg\">\n",
    "\n",
    "\n",
    "In Timestep2 we choose another epsilon greedy strategy, this time the epsilon is 0.99 as the more we train the less exploration we will do.\n",
    "We take action down, where the poison is. Reward is -10.\n",
    "In this case we update the table with this action.\n",
    "\n",
    "<img src=\"pics/UpdateQ3.jpg\">\n",
    "\n",
    "Even though we died, the agent is already becoming smarter. The more we explore and exploit the environment and update the Q-values using TD target, the Q-table will give us better approximations so that by the end of training we will have an optimal Q-function. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
