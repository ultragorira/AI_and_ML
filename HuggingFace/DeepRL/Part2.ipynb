{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All pics are taken from https://huggingface.co/blog/deep-rl-q-part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Based method\n",
    "\n",
    "Indirectly, by training a value function that outputs the value of a state or a state-action pair. Given this value function, the policy will take action.\n",
    "\n",
    "In Value Based methods we do not train the policy, the policy is a function defined by hand. We train a value function that is a Neural Network. \n",
    "\n",
    "So no matter the method, you will still have a policy but in case in value mathods, the policy is just a function that is specified, as a greedyt policy. \n",
    "\n",
    "Main differences:\n",
    "\n",
    "In policy based, the optimal policy is found by training the policy directly.\n",
    "In value based, finding an optimal value function leads to having an optimal policy.\n",
    "\n",
    "In most cases, an Epsillon-Greedy Policy is used to handle the exploration and exploitation.\n",
    "\n",
    "### Two type of value based functions\n",
    "\n",
    "***State-Value function***\n",
    "For each state, the state value function outputs the expected return if the agent starts at that state and then follow the policy forever after.\n",
    "\n",
    "<img src='pics/state-value-function-1.jpg'>\n",
    "\n",
    "The state value function calculates the value of a state\n",
    "\n",
    "***Action-Value function***\n",
    "\n",
    "For each state and action pair, the action value function outputs the expected return if the agent starts in that state and takes action, and then follows the policy forever after. \n",
    "\n",
    "<img src='pics/action-state-value-function-1.jpg'>\n",
    "\n",
    "\n",
    "### Main difference between State Value and Action Value functions ###\n",
    "\n",
    "In State Value Function we calculate the value of the state St\n",
    "In Action Value Function we calculate the value of the state-action pair St and At, so the value of taking the action at that state. \n",
    "\n",
    "<img src='pics/two-types.jpg'>\n",
    "\n",
    "In both we calculate the expected value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation ###\n",
    "\n",
    "For both State and Action Value based functions, we need to calculate for each of the state or action-state pair, the sum of all the rewards an agent can get if the agent started at that state. This can be a very lenghty process and the Bellman Equation comes in handy.\n",
    "\n",
    "\n",
    "The Bellman Equation is a recurisve equation.\n",
    "\n",
    "<img src='pics/bellman.jpg'>\n",
    "\n",
    "Immediate reward Rt+1 plus the discounted value of the state that follows (gamma * V(St+1))\n",
    "\n",
    "The main idea is that instead of calculating each value as the sum of the expected return, we calculate the sum of the immediate reward and the discounted value of the state that follows. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo vs TDL (Temporal Difference Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo and TDL are two ways of training the value function or the policy function. Both of them use experience to solve RL problems.\n",
    "\n",
    "With Monte Carlo we use an entier episode of experience before learning, while TDL uses only a step to learn.\n",
    "\n",
    "##### Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pics/monte-carlo-approach.jpg'>\n",
    "\n",
    "With Monte Carlo we always start at the same starting point. The agent takes actions based on the policy.\n",
    "We check if the episode is finished, e.g. mouse got eaten by cat, and at the end of the episode we will have a list of States, Actions, Rewards and Next steps.\n",
    "The agent will sump the total rewards Gt and see how good it did.\n",
    "V(st) in the formula will be updated.\n",
    "\n",
    "Start new episode with this new knowledge. The more episodes, the more the agent will learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TDL Temporal Distance Learning\n",
    "\n",
    "In TDL, we do not wait for a full episode to finish but wait only for one interaction/step St+1\n",
    "\n",
    "In TDL we update V(St) at each step.\n",
    "However since we did not go through a full episode, we do not have Gt (expected return) but we estimate Gt by adding Rt+1 and the discounted value of the next step.\n",
    "\n",
    "<img src='pics/TDL.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Monte Carlo we have the actual accurate discounted return of the episode and update the function only after the episode is complete. This\n",
    "\n",
    "With TDL, the function is updated at each step and replace Gt (expected return) with an estimated return called TD target.\n",
    "\n",
    "<img src='pics/Difference_MonteCarlo_TDL.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is an off policy value-based method that uses Temporal Distance approach to train its action-value function. \n",
    "Q-Learning algorithm is used to train the Q-function which is action-bases (pair of action and state) to determined the value of being at a particular state and taking a specific action at that state. \n",
    "Q is meant for Quality of the action of that state. \n",
    "\n",
    "Given a pair of state and action, the Q function will output a state-action value, called also Q-value.\n",
    "The function has a Q-table, which acts like memory of the function where each cell corresponds to a state-action value pair. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
