{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-LEARNING\n",
    "All pics taken from \"https://huggingface.co/blog/deep-rl-dqn\"\n",
    "\n",
    "With Deep Q-Learning, there is no Q-Table but a Neural Network.\n",
    "\n",
    "In Q-Learning, we train the Q-Function (action value function) which determines the value of being at a particular state and taking specific action at that state.\n",
    "\n",
    "The Q-Table is a table where each cell corresponds to a state-action pair value. Like a memory or cheat-sheet.\n",
    "Q-Learning is tabular method, which can be used where state and action spaces are small enough to approximante value functions to be represented as tables. ***Not scalable***\n",
    "\n",
    "<img src=\"pics/Deep-Q-Learning.jpg\">\n",
    "\n",
    "### Deep Q-Network\n",
    "\n",
    "<img src=\"pics/deep-q-network.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input will be a stack of 4 frames.\n",
    "Then we will use the epsilon-greedy policy to select which action to perform.\n",
    "\n",
    "We will downscale the frames of the game from RGB to Grayscale so that we have 1 channel instead of 3, as colors do not really matter for Atari games. And we will also reduce the size.\n",
    "\n",
    "<img src=\"pics/RGB_To_GrayScale.jpg\">\n",
    "\n",
    "Frames are stacked together to help with the temporal limitation. With just 1 frame we would not know for example where an object is moving to, what direction but if you had more frames you can evaluate the right direction:\n",
    "\n",
    "<img src=\"pics/Temporal_Limitation.jpg\">\n",
    "\n",
    "The data is then passed to CNN layers and finally to a fully connected layer which will output which action to take.\n",
    "\n",
    "With Deep Q-Learning we have a Loss function which calculates the difference between Q-value predicted and Q-target and use Gradient descent to update the weights ot the NN. \n",
    "\n",
    "\n",
    "The training will have two phases:\n",
    "\n",
    "1. Sampling: perform actions and store observed experiences tuples in a replay memory\n",
    "2. Training: select small batch of tuple randomly. Learn from using gradient descent\n",
    "\n",
    "<img src=\"pics/sampling-training.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep-Q Learning can suffer from instability. To obviate this, 3 solutions to implement:\n",
    "\n",
    "Experience Replay\n",
    "Fixed Q-Target\n",
    "Double Deep Q-Learning\n",
    "\n",
    "\n",
    "Experience Replay helps to make more efficient the experience during the training phase. With Experience Replay we create a buffer that saves samples of the experience and can be reused during the training. So we will be able to learn from individual experiences multiple times. \n",
    "\n",
    "The Replay Buffer stores experiences tuples while interacting with the environment and then samplew a small batch of tuples. This will prevent the network from learning only from what has been immediately done. \n",
    "\n",
    "<img src=\"pics/experience-replay.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
